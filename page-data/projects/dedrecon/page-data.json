{"componentChunkName":"component---src-pages-markdown-remark-frontmatter-slug-js","path":"/projects/dedrecon/","result":{"data":{"markdownRemark":{"html":"<h3><a href=\"https://cvcweb.oden.utexas.edu/cvcwp/projects/dedrecon/\">Computational Visualization Center Wordpress Link</a></h3>\n<h2>Project Components</h2>\n<ul>\n<li><a href=\"../video-imputation\">Video Imputation</a></li>\n<li><a href=\"../sample-complexity\">Sample Complexity</a></li>\n</ul>\n<h2>What are we trying to do?</h2>\n<p>The goal of this project was to <strong>develop (train, validate, and test)</strong> advanced <strong>machine learning</strong> and <strong>deep learning</strong> technologies for <strong>intelligent, real-time stream processing</strong> of diverse <strong>Electro-Optical (EO) / Infrared (IR)</strong> video streams, commonly referred to as hyperspectral or EO/IR imagery, to enable <strong>wide area persistent surveillance</strong>. When paired with EO/IR video sensor systems, these analytic software technologies become essential components of the Army’s next-generation active protection systems.</p>\n<p>By <strong>developing</strong> these technologies, we have:</p>\n<ol>\n<li>Documented <strong>spatio-temporal-spectral (STS)</strong> details of EO/IR data,</li>\n<li>Identified which STS phenomena are discernible or discriminable,</li>\n<li>Detected and characterized STS anomalies (i.e., unexpected or unusual spatio-temporal-spectral patterns or changes).</li>\n</ol>\n<h2>What is the problem?</h2>\n<p>We addressed the challenge of <strong>detecting various types of object and motion anomalies</strong> using different spatial and spectral resolution hyperspectral video systems under a wide range of lighting and weather conditions.</p>\n<h2>Why is it hard?</h2>\n<p>Scenes under surveillance can rapidly change in appearance and content. Multiple and diverse objects may be moving at different distances from the sensors, and environmental conditions (time of day, weather, terrain, seasons) can vary drastically. Each scenario can demand unique, “intelligent” data analytics that adapt to different sensor systems.</p>\n<h2>How is it done today, and what are the limits of current practice?</h2>\n<p>A standard approach to anomaly detection relies on <strong>optical flow</strong>—the movement of colors in a video—to detect motion. Pixels flagged by optical flow are passed through a conventional neural network to decide if they are anomalous. However, this approach:</p>\n<ul>\n<li>Is <strong>computationally expensive</strong> (optical flow can be costly to compute),</li>\n<li>Is <strong>imprecise</strong>, often marking too many pixels or missing smaller anomalies,</li>\n<li>Is <strong>scenario-specific</strong>, usually tested under restricted conditions (e.g., one time of day).</li>\n</ul>\n<p>We overcame these limitations by developing <strong>multiply learned deep neural network models</strong> that perform robustly <strong>day or night</strong>, across <strong>all seasons</strong>, and under <strong>any weather conditions</strong>.</p>\n<h2>What we’re doing</h2>\n<p>Our approach involved <strong>training, validating, and testing</strong> variational autoencoders—referred to as <strong>Deep Encoder-Decoder Recurrent Networks (DEDRECON)</strong>—to model “normal” through “extra-ordinary” motion and patterns in a variety of realistic environmental scenarios. Once trained, these DEDRECON models <strong>detect and classify object-scene patterns</strong> and identify motion anomalies in real time, labeling them as deviations from learned “normal” behavior and categorizing them into levels of “extra-ordinary,” i.e., potential threats.</p>\n<p>Specifically, we processed two distinct EO/IR video streams:</p>\n<ol>\n<li>A <strong>hyperspectral</strong> stream with <strong>high spectral resolution</strong>,</li>\n<li>A <strong>multispectral</strong> stream with <strong>high spatial resolution</strong>.</li>\n</ol>\n<p>We trained two separate variational autoencoders—one for each stream—and then combined them within a <strong>common correlation space</strong> via two <strong>invertible recurrent neural networks</strong>. This architecture:</p>\n<ul>\n<li>Maximizes <strong>discernibility</strong> of objects, scenes, motion, and environmental variations,</li>\n<li>Captures finer details in anomalies across multiple locations,</li>\n<li>Extends detection range when paired with appropriate lenses,</li>\n<li>Produces fewer “false alarms” by filtering out unnecessary anomalous pixels.</li>\n</ul>\n<h2>On-the-Fly Compression and Sampling</h2>\n<p><strong>SketchyCoreSVD</strong> is an on-the-fly sampling and compression method designed for matrices (e.g., 2D images). References [4,5] provide examples of this technique. In this project, we <strong>extended</strong> on-the-fly sampling and compression to <strong>time-varying multimedia data</strong>, treating them as <strong>tensors</strong> of order greater than three (note: tensors of order two are matrices, order one are vectors, and order zero are scalars). We have released <strong>MATLAB</strong> and <strong>Python</strong> implementations of these methods.</p>\n<h2>Sensing Modalities</h2>\n<p>We worked with multiple sensing modalities, including:</p>\n<ul>\n<li><strong>Hyperspectral Imaging (HSI)</strong>,</li>\n<li><strong>Electro-Optical (EO) / Infrared (IR)</strong>,</li>\n<li><strong>Radio Frequency (RF)</strong>,</li>\n<li><strong>Light Detection and Ranging (LiDAR)</strong>.</li>\n</ul>\n<p>Leveraging multiple modalities exploits the strengths of each while also posing challenges in <strong>data fusion</strong> and <strong>on-the-fly compression</strong>.</p>\n<h2>PROGRESS</h2>\n<p>We successfully achieved <strong>Super Resolution for Enhanced Identification of Target Regions of Interest (TROI)</strong> by combining two low-resolution multispectral–hyperspectral video streams [1,2,3].</p>\n<h2>Publications</h2>\n<ol>\n<li>C. Bajaj, Y. Wang, <em>Super-Resolution Tensor Recovery from Multi-Level Adaptive Sub-Sampling</em>, Manuscript 2020.</li>\n<li>C. Bajaj, Y. Wang, <em>Blind Hyperspectral-Multispectral Image Fusion via Graph Laplacian Regularization</em>, arXiv:1902.08224.</li>\n<li>L. Wu, X. Li, C. Bajaj, <em>Dynamic Filtering with Large Sampling Field for ConvNets</em>, <em>European Conference on Computer Vision (ECCV)</em> 2018, <a href=\"https://doi.org/10.1007/978-3-030-01249-6_12\">https://doi.org/10.1007/978-3-030-01249-6_12</a>.</li>\n<li>S. Gupta, C. Bajaj, <em>A Streaming Model for Generalized Rayleigh with Extensions to Minimum Noise Fraction</em>, <em>Proceedings of IEEE Big Data Conference</em> 2019, <a href=\"https://doi.org/10.1109/BigData47090.2019.9006512\">https://doi.org/10.1109/BigData47090.2019.9006512</a>.</li>\n<li>C. Bajaj, Y. Wang, T. Wang, <em>SketchyCoreSVD: SketchySVD from Random Subsampling of the Data Matrix</em>, <em>Proceedings of IEEE Big Data Conference</em> 2019, <a href=\"https://doi.org/10.1109/BigData47090.2019.9006345\">https://doi.org/10.1109/BigData47090.2019.9006345</a>.</li>\n</ol>\n<h2>Funding</h2>\n<p>This project is funded by <strong>Army Futures Command (AFC)</strong>.</p>\n<h2>People</h2>\n<ul>\n<li><strong>Principal Investigator (PI):</strong> Prof. Chandrajit Bajaj</li>\n<li>Ryan Farell (Project Lead)</li>\n<li>Ahmed Abdelkader (Postdoc)</li>\n<li>Luke McLennan (PhD)</li>\n<li>Chase Tessmer (PhD)</li>\n<li>Yi Wang (PhD)</li>\n<li>Nikhil Ajjarapy (Undergraduate Researcher)</li>\n<li>Contrad Li (Undergraduate Researcher)</li>\n<li>Benjamin Beal (Undergraduate Researcher)</li>\n<li>Edward Zhou (Undergraduate Researcher)</li>\n</ul>","frontmatter":{"title":"Deep Encoder-Decoder Recurrent Networks"}}},"pageContext":{"id":"1a95351c-3c73-5d97-8981-14acfec27352","frontmatter__slug":"/projects/dedrecon","__params":{"frontmatter__slug":"projects"}}},"staticQueryHashes":["4166265360"],"slicesMap":{}}